{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# xSDR: Speaker-Independent Spoken Digit Recognition\n",
    "This is our solution for the final graded project for the WS22/23 course \"_Neural Networks: Theory and Implementation_\" at Saarland University.\n",
    "<br/>\n",
    "Authors: _Christian Singer, Mhd Jawad Al Rahwanji_\n",
    "`{chsi00002, mhal00002}@stud.uni-saarland.de`\n",
    "<br/>\n",
    "<img src=\"xSDR.png\" height=333>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Before we start, we'd like to point out a couple of things:\n",
    "\n",
    "-- An **Introduction** to the project can be found in `README.md`\n",
    "\n",
    "-- Some preliminary **Data Exploration** can be found in `DataExploration.ipynb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# add this to ignore warnings from Librosa\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import torchmetrics as tm\n",
    "\n",
    "from comparative_analysis.tsne_model_embeddings import tsne_model, plot_tsne\n",
    "from model_baseline.data_loading import create_features\n",
    "from torch import nn\n",
    "from model_baseline.linear_model import classifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "from model_neural.classification_report import eval_models\n",
    "from model_neural.utils.data_loading import MNISTAudio\n",
    "from model_neural.utils.helpers import annotations_dir, base_dir\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task I"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# I.1\n",
    "\n",
    "# Original definition can be found in: model_baseline.data_loading.py\n",
    "def downsample_spectrogram(spectrogram, num_frames):\n",
    "    \"\"\"\n",
    "    Given a mel-scaled representation of a signal, return a fixed-size\n",
    "    representation of the signal as numpy array of size (1, num_frames)\n",
    "    by taking num_frames equal sized chunks of the signal and averaging\n",
    "    them over the frequency axis.\n",
    "    \"\"\"\n",
    "\n",
    "    signal_length = spectrogram.shape[1]\n",
    "    window_size = int(math.ceil(spectrogram.shape[1] / num_frames))\n",
    "    padding = num_frames * window_size - signal_length\n",
    "\n",
    "    spectrogram_downsampled = np.zeros((spectrogram.shape[0], num_frames))\n",
    "\n",
    "    # pad signal with zeros\n",
    "    spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), \"constant\")\n",
    "\n",
    "    for section in range(num_frames):\n",
    "        spectrogram_downsampled[:, section] = np.mean(\n",
    "            spectrogram[:, section * window_size : (section + 1) * window_size], axis=1\n",
    "        )\n",
    "\n",
    "    spectrogram_downsampled = np.reshape(spectrogram_downsampled, (1, -1))\n",
    "    return spectrogram_downsampled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# I.2\n",
    "\n",
    "# Original transformation can be found in: model_baseline.linear_model.py\n",
    "# Uses downsample_spectrogram(...) as well as extract_melspectrogram(...)\n",
    "trnf, trnl = create_features(\"TRAIN\")\n",
    "devf, devl = create_features(\"DEV\")\n",
    "tstf, tstl = create_features(\"TEST\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('sgdclassifier',\n                 SGDClassifier(loss='modified_huber', n_jobs=-1,\n                               penalty='elasticnet'))])",
      "text/html": "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;sgdclassifier&#x27;,\n                 SGDClassifier(loss=&#x27;modified_huber&#x27;, n_jobs=-1,\n                               penalty=&#x27;elasticnet&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;sgdclassifier&#x27;,\n                 SGDClassifier(loss=&#x27;modified_huber&#x27;, n_jobs=-1,\n                               penalty=&#x27;elasticnet&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;modified_huber&#x27;, n_jobs=-1, penalty=&#x27;elasticnet&#x27;)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I.3\n",
    "\n",
    "# Original fitting can be found in: model_baseline.linear_model.py\n",
    "# Both penalty and loss parameters were experimented with,\n",
    "# \"elasticnet\" and \"modified_huber\" were chosen, respectively.\n",
    "classifier.fit(trnf, trnl)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------DEV-SET-----------------------------------------\n",
      "Confusion matrix:\n",
      "[[39  1  0  1  2  0  0  4  0  0]\n",
      " [ 0 17  0  0 19  4  0  0  1  4]\n",
      " [ 6  0 26  6  0  0  0  7  0  1]\n",
      " [ 0  0  2 26  0  3  0 12  9  2]\n",
      " [ 5  1  0  0 34  4  0  6  0  4]\n",
      " [ 0  4  0  0  1 37  0  6  0  2]\n",
      " [ 1  0  2  3  0  0  3  5 39  1]\n",
      " [ 0  0  0  0  0  8  0 43  1  1]\n",
      " [ 0  0  2  0  0  3  0  0 38  1]\n",
      " [ 0  9  0  0  2 13  0  4  0 22]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80        47\n",
      "           1       0.53      0.38      0.44        45\n",
      "           2       0.81      0.57      0.67        46\n",
      "           3       0.72      0.48      0.58        54\n",
      "           4       0.59      0.63      0.61        54\n",
      "           5       0.51      0.74      0.61        50\n",
      "           6       1.00      0.06      0.11        54\n",
      "           7       0.49      0.81      0.61        53\n",
      "           8       0.43      0.86      0.58        44\n",
      "           9       0.58      0.44      0.50        50\n",
      "\n",
      "    accuracy                           0.57       497\n",
      "   macro avg       0.64      0.58      0.55       497\n",
      "weighted avg       0.65      0.57      0.54       497\n",
      "\n",
      "\n",
      "---------------------------------TEST_SET------------------------------------------\n",
      "Confusion matrix:\n",
      "[[41  0  1  0  2  0  0  9  0  0]\n",
      " [ 0 22  0  0 15  6  0  2  1  9]\n",
      " [ 9  0 30  5  0  0  0  6  4  0]\n",
      " [ 0  0  2 22  0  8  0  4  8  2]\n",
      " [ 9  1  0  0 27  4  0  5  0  0]\n",
      " [ 0  9  0  0  1 34  0  3  0  3]\n",
      " [ 0  0  3  5  0  2  4  2 30  0]\n",
      " [ 0  0  1  0  0  1  1 43  1  0]\n",
      " [ 0  0  2  0  0  1  1  0 52  0]\n",
      " [ 0  3  0  0  0 14  0  4  1 28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73        53\n",
      "           1       0.63      0.40      0.49        55\n",
      "           2       0.77      0.56      0.65        54\n",
      "           3       0.69      0.48      0.56        46\n",
      "           4       0.60      0.59      0.59        46\n",
      "           5       0.49      0.68      0.57        50\n",
      "           6       0.67      0.09      0.15        46\n",
      "           7       0.55      0.91      0.69        47\n",
      "           8       0.54      0.93      0.68        56\n",
      "           9       0.67      0.56      0.61        50\n",
      "\n",
      "    accuracy                           0.60       503\n",
      "   macro avg       0.63      0.60      0.57       503\n",
      "weighted avg       0.63      0.60      0.58       503\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.4\n",
    "\n",
    "# Original evaluation can be found in: model_baseline.linear_model.py\n",
    "print(\"----------------------------------DEV-SET-----------------------------------------\")\n",
    "dev_preds = classifier.predict(devf)\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(devl, dev_preds)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(devl, dev_preds)}\\n\")\n",
    "print(\"---------------------------------TEST_SET------------------------------------------\")\n",
    "test_preds = classifier.predict(tstf)\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(tstl, test_preds)}\\n\")\n",
    "print(f\"Classification Report:\\n{classification_report(tstl, test_preds)}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task II"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# II.1\n",
    "\n",
    "# Answers to subquestions first:\n",
    "# Yes, both neural models outperformed our baseline model.\n",
    "# TODO: Await latest model evaluations...\n",
    "# TODO: Fill with answer to:  \"Do you observe any signs of overfitting to the training data?\"\n",
    "# TODO: Fill with answer to:  \"How do the hyperparameters affect the model performance?\"\n",
    "# TODO: Discuss the above observations after reporting them\n",
    "\n",
    "# Original implementations can be found in: model_neural.(conv1d_model.py & transformer_model.py)\n",
    "class conv1d_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, pool_size=4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1dModel(nn.Module):\n",
    "    \"\"\"Convolutional neural network for 1D data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input=1,\n",
    "        n_channel=32,\n",
    "        n_output=10,\n",
    "        initial_kernel_size=60,\n",
    "        initial_stride=8,\n",
    "        final_pool_size=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = conv1d_block(n_input, n_channel, initial_kernel_size, initial_stride)\n",
    "        self.conv_block2 = conv1d_block(n_channel, n_channel)\n",
    "        self.conv_block3 = conv1d_block(n_channel, 2 * n_channel)\n",
    "        self.conv_block4 = conv1d_block(2 * n_channel, 2 * n_channel, pool_size=final_pool_size)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1dMelModel(nn.Module):\n",
    "    \"\"\"Convolutional neural network for 1D data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_input=39, n_channel=32, n_output=10, initial_kernel_size=16, initial_stride=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = conv1d_block(\n",
    "            n_input, n_channel, initial_kernel_size, initial_stride, pool_size=2\n",
    "        )\n",
    "        self.conv_block2 = conv1d_block(n_channel, n_channel, pool_size=1)\n",
    "        self.conv_block3 = conv1d_block(n_channel, 2 * n_channel, pool_size=1)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "\n",
    "    def __init__(self, num_hiddens, dropout, max_len=150):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        mask = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens\n",
    "        )\n",
    "        # 0::2 means even indices, 1::2 means odd indices.\n",
    "        self.P[:, :, 0::2] = torch.sin(mask)\n",
    "        self.P[:, :, 1::2] = torch.cos(mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.P[:, : x.shape[1], :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, mlp_num_hidden, mlp_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(mlp_num_hidden)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.dense2(self.dropout1(self.gelu(self.dense1(x)))))\n",
    "\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, num_hidden, norm_shape, mlp_num_hidden, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(norm_shape)\n",
    "        self.attention = nn.MultiheadAttention(num_hidden, num_heads, 0.1, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(norm_shape)\n",
    "        self.mlp = ViTMLP(mlp_num_hidden, num_hidden)\n",
    "\n",
    "    def forward(self, x, valid_lens=None):\n",
    "        x = x + self.attention(*([self.ln1(x)] * 3))[0]\n",
    "        return x + self.mlp(self.ln2(x))\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=16,\n",
    "        stride=10,\n",
    "        num_hidden=128,\n",
    "        mlp_num_hidden=128,\n",
    "        num_heads=4,\n",
    "        num_blocks=2,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = nn.LazyConv2d(num_hidden, kernel_size=patch_size, stride=stride)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_hidden))\n",
    "        self.pos_embedding = PositionalEncoding(num_hidden, 0.1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.add_module(\n",
    "                f\"{i}\", ViTBlock(num_hidden, num_hidden, mlp_num_hidden, num_heads)\n",
    "            )\n",
    "        self.head = nn.Sequential(nn.LayerNorm(num_hidden), nn.Linear(num_hidden, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add channel dimension.\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.patch_embedding(x).flatten(2).transpose(1, 2)\n",
    "        # Using .expand adds cls token for each sample in the batch.\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(x)[:, 0, :].unsqueeze(1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# II.2\n",
    "\n",
    "# TODO: Also await latest model evaluations...\n",
    "# We have the classification report for the baseline model\n",
    "# Now, we produce the classification report for the best models\n",
    "\n",
    "# Original evaluation can be found in neural_model.classification_report.py\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: '{device}' as device for report.\")\n",
    "\n",
    "model = Conv1dModel()\n",
    "model.load_state_dict(torch.load(\"models/Conv1dModel_0002_0002_10_01.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Conv1dModel doesn't use mel-spectrogram, so we need to specify that.\n",
    "if model.__class__.__name__ in [\"TransformerModel\", \"Conv1dMelModel\"]:\n",
    "    to_mel = True\n",
    "else:\n",
    "    to_mel = False\n",
    "\n",
    "report = eval_models(model, [\"TRAIN\", \"DEV\", \"TEST\"], device=device, to_mel=to_mel)\n",
    "\n",
    "# TODO: Compare the classification reports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# II.3\n",
    "\n",
    "# Original usage can be found in: comparative_analysis.tsne_model_embeddings.py\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: '{device}' as device for report.\")\n",
    "\n",
    "model = Conv1dModel()\n",
    "model.load_state_dict(torch.load(\"../model_neural/models/Conv1dModel_0002_0002_10_01.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if model.__class__.__name__ in [\"TransformerModel\", \"Conv1dMelModel\"]:\n",
    "    to_mel = True\n",
    "else:\n",
    "    to_mel = False\n",
    "\n",
    "tsne_embedding, labels = tsne_model(model, device, to_mel, split=\"DEV\")\n",
    "plot_tsne(tsne_embedding, labels)\n",
    "\n",
    "# TODO: Do the same for linear model\n",
    "\n",
    "# TODO: Compare to the baseline after reporting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# II.4\n",
    "\n",
    "# Original implementation can be found in: comparative_analysis.statistical_significance_test.py\n",
    "def test_statistical_significance(model, baseline, device: torch.device):\n",
    "    \"\"\"Test for statistical significance between models w.r.t accuracy\n",
    "    :return: p-value\"\"\"\n",
    "    # Adapted from https://aclanthology.org/D12-1091.pdf\n",
    "    # Sample with replacement for val (DEV) set\n",
    "    ds = MNISTAudio(annotations_dir=annotations_dir, audio_dir=base_dir, split=\"DEV\", to_mel=True)\n",
    "    n = ds.__len__()\n",
    "    s = 0\n",
    "    b = 10 ** 6\n",
    "\n",
    "    if issubclass(model, nn.Module):\n",
    "        model_accuracy_metric = tm.classification.MulticlassAccuracy(num_classes=10)\n",
    "        model_accuracy_metric.to(device)\n",
    "    if issubclass(baseline, nn.Module):\n",
    "        baseline_accuracy_metric = tm.classification.MulticlassAccuracy(num_classes=10)\n",
    "        baseline_accuracy_metric.to(device)\n",
    "\n",
    "    for _ in range(b):\n",
    "        sampler = RandomSampler(ds, replacement=True, num_samples=n)\n",
    "        dl = DataLoader(ds, sampler=sampler, batch_size=32)\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(dl):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                if issubclass(model, nn.Module):\n",
    "                    model_output = model(data)\n",
    "                    model_pred = model_output.argmax(dim=2, keepdim=True).squeeze()\n",
    "                else:\n",
    "                    model_pred = model.predict(data)\n",
    "                model_accuracy_metric(model_pred, target)\n",
    "\n",
    "                if issubclass(baseline, nn.Module):\n",
    "                    baseline_output = baseline(data)\n",
    "                    baseline_pred = baseline_output.argmax(dim=2, keepdim=True).squeeze()\n",
    "                else:\n",
    "                    baseline_pred = baseline.predict(data)\n",
    "                baseline_accuracy_metric(baseline_pred, target)\n",
    "\n",
    "        model_accuracy = model_accuracy_metric.compute()\n",
    "        model_accuracy_metric.reset()\n",
    "        baseline_accuracy = baseline_accuracy_metric.compute()\n",
    "        baseline_accuracy_metric.reset()\n",
    "\n",
    "        if model_accuracy > baseline_accuracy:\n",
    "            s += 1\n",
    "\n",
    "    return s / b\n",
    "\n",
    "# TODO: Compare and report all 3 models after pairwise signif. testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task III"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# III.1\n",
    "\n",
    "# Answers to subquestions first:\n",
    "# TODO: Await latest model evaluations...\n",
    "# TODO: Fill with answer to:  \"What do you observe?\"\n",
    "# TODO: Fill with answer to:  \"How does this affect the model performance?\"\n",
    "# TODO: Discuss the above observations after reporting them\n",
    "\n",
    "# TODO: Retrain all 3 using new dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# III.2\n",
    "\n",
    "# TODO: Complete."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# III.3\n",
    "\n",
    "# TODO: Complete."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
