{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454e0124",
   "metadata": {
    "id": "454e0124"
   },
   "source": [
    "# Speaker-Independent Spoken Digit Recognition (xSDR)\n",
    "\n",
    "\n",
    "One of the successful stories of deep neural networks is the proliferation of commercial of automatic speech recognition (ASR) systems. This project aims to explore one application of ML-powered ASR to the problem of spoken digit recognition (SDR). Since digits are widely used as unique identifiers for bank information, social security numbers, post codes, etc, SDR systems can be an efficient alternative to fully-fledged ASR systems since the domain is more predictable than other applications of ASR. \n",
    "\n",
    "In this project, we focus on developing a SDR system in a speaker-independent setting. That is, the speakers in the evaluation set are disjoint from the training set speakers. We do so because we expect real-world ASR systems to generalize to different speakers than those we have data for. Moreover, for many languages that are under-resourced, we have have (limited) annotated speech data from a single speaker, but we would still want the system to be deployed to work on any speaker of that language. We tackle the problem of spoken digit recognition as a sequence classification task. Concretely, the inputs are short audio clips of a specific digit (in the range 0-9), then the goal is to build deep neural network models to classify a short audio clip and predict the digit that was spoken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83711d-f753-44e7-9521-8881623b9521",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Code Submission\n",
    "\n",
    "You don't necessarily need to complete the code in this Jupyter Notebook, you are free to use another notebook or a python script file, as you would like. You are expected to submit the code by **22.03.2023**.\n",
    "\n",
    "Your code should be clean and well commented. We also expect that if we decide to run it on our system, it should be straighforward to do so. We recommend creating a ```requirements.txt``` file with the names of all the libraries with their versions. If applicable, please mention the python version in a ```README.md``` file, which should also include instructions on how to run your code.\n",
    "\n",
    "As mentioned for the assignments, always remember to cite the code with the links as comments, if you decide to use it from a public repository.\n",
    "\n",
    "## Report Submission\n",
    "\n",
    "With the code, you are also expected to submit a report with a maximum of 4 pages. You should write your report in LaTeX using this template for ACL 2023 [Overleaf Link](https://www.overleaf.com/latex/templates/acl-2023-proceedings-template/qjdgcrdwcnwp). Use this document to fill in any missing information that are not necessarily covered during your presentation for the sake of time in the presentation. While writing your report, we would highly encourgae you to cite the papers behind each tool / library / function that you might use for your experiments. We have also released an example on how to write equations in LaTeX [here](https://piazza.com/class/l9so16qqvk34hu/post/52).\n",
    "\n",
    "You art also expected to submit this report with your code. You should provide the **.tex, .pdf and all image files** zipped with the same naming convention as it was in your assignment(s).\n",
    "\n",
    "## Presentation\n",
    "\n",
    "During the last week of March 2023, i.e. 27.03 -- 31.03, each team will be presenting their works for 15 minutes. We expect equal contribution from each member in delivery and content of the presentation. So roughly 5 minutes for one person, if you have 3 people in your team. There will be 5 minutes for some Q&A. At-least one person from your team should be present to do an in-person presentation, rest of your team could join remotely, if they are not present.\n",
    "\n",
    "## Important Dates\n",
    "\n",
    " - Code & Report Submission: 22.03.2023 (08.00)\n",
    " - Presentation: 27.03.2023 -- 31.03.2023\n",
    " \n",
    " You'll get a precise date and time for your team's presentation at a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61601679-f2f0-437a-8233-54c5ab0cac17",
   "metadata": {},
   "source": [
    "### Grading\n",
    "\n",
    "In this project, your final grades will be determined as follows:\n",
    "\n",
    " - **30%**: &emsp; Completing all the tasks\n",
    " - **30%**: &emsp; Providing scientific-backings for all the methods used\n",
    " - **20%**: &emsp; Quality of the content of the presentation\n",
    " - **20%**: &emsp; Delivery of the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b704f",
   "metadata": {
    "executionInfo": {
     "elapsed": 3612,
     "status": "ok",
     "timestamp": 1674212543207,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "020b704f"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "import librosa, librosa.display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn  import preprocessing\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# add this to ignore warnings from Librosa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2719f5",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1674212543209,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "2b2719f5"
   },
   "outputs": [],
   "source": [
    "# for linear models \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e4098",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1674212543209,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "f70e4098"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a0884",
   "metadata": {
    "id": "6a0a0884"
   },
   "source": [
    "## Exploring the Dataset \n",
    "\n",
    "The speech samples are already divied into training, development, and test spilts. The splits are made in such way that evaluation speakers are not present in training split. You should use the splits as they are. \n",
    "\n",
    "**CAUTION:** \n",
    "\n",
    "In this project, you are not allowed to use any external data for this problem (at least for the main three tasks). Exploring the effect of additional datasets in this project can only included as a further step after completing the main requirements with the given data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40503e1",
   "metadata": {
    "id": "c40503e1"
   },
   "outputs": [],
   "source": [
    "# read tsv file into a dataframe \n",
    "sdr_df = pd.read_csv('SDR_metadata.tsv', sep='\\t', header=0, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a087bc",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1674212543211,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "b9a087bc"
   },
   "outputs": [],
   "source": [
    "sdr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34786a",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1674212543212,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "4c34786a"
   },
   "outputs": [],
   "source": [
    "set(sdr_df.speaker.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ea375",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "aborted",
     "timestamp": 1674212543213,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "155ea375"
   },
   "outputs": [],
   "source": [
    "# explore one sample: 7_theo_0\n",
    "sdr_df.loc[sdr_df['identifier'] == '7_theo_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e0920",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1674212543214,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "e03e0920"
   },
   "outputs": [],
   "source": [
    "sample_wav_file = sdr_df.loc[sdr_df['identifier'] == '7_theo_0'].file[700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5f7e7",
   "metadata": {
    "id": "6ab5f7e7"
   },
   "source": [
    "## The Speech Waveform\n",
    "\n",
    "The acoustic realization of speech segment can be (digitally) viewed as a time-variant wavform $\\mathbf{S} \\in \\mathbb{R}^{n}$. Here, $n$ depends on both the duration of the speech segment and the sampling rate of the continous speech singal. Let's check out one sample from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93353b",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1674212543214,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "ac93353b"
   },
   "outputs": [],
   "source": [
    "# play and listen to a sample \n",
    "SAMPLING_RATE = 8000 # This value is determined by the wav file, DO NOT CHANGE\n",
    "\n",
    "x, sr = librosa.load(sample_wav_file, sr=SAMPLING_RATE) #, \n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7438910",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "aborted",
     "timestamp": 1674212543215,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "b7438910"
   },
   "outputs": [],
   "source": [
    "# plot as a waveform \n",
    "fig, ax = plt.subplots(figsize=(10, 2), sharex=True)\n",
    "\n",
    "img = librosa.display.waveshow(y=x, sr=sr, alpha=0.75, x_axis='time', color='blue')\n",
    "\n",
    "ax.set(title='Amplitude waveform')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4297e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1674212543216,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "3c4297e5"
   },
   "outputs": [],
   "source": [
    "# sample duration in milliseconds\n",
    "(1000*len(x))/SAMPLING_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df44120",
   "metadata": {
    "id": "3df44120"
   },
   "source": [
    "In the cell above, you can see the temporal duration of the audio is 428.5 milliseconds. For digits in the range 0-9, the duration of the speech segment should be around 0.5 seconds with reasonable variation depending on speech rate (i.e., how fast the speaker speaks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0c08e",
   "metadata": {
    "id": "d2a0c08e"
   },
   "source": [
    "## The Speech Signal Representation - Mel Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370d9c2",
   "metadata": {
    "id": "6370d9c2"
   },
   "source": [
    "Humans can recognize and differentiate different speech sounds based on the frequency characteristics of the sounds. For machine learning applications, human speech is represented using spectro-temporal features in the [Mel-scale](https://en.wikipedia.org/wiki/Mel_scale) extracted from the speech sample. Mel-scale features are inspired by human speech perception and auditory processing whereby the human ear has difference sensitivity (or resolution) in differet frequency bandes. That is, the human ear can better recognize differences in in lower range frequences, while higher range frequences have a lower resolution. The Mel-scale is linear for frequencies in the range (0-1kHz), and logarithmic for frequencies above 1kHz.\n",
    "\n",
    "In the spectro-temporal representation of speech, a speech sample can be seen as a sequence of $T$ spectral vectors as $\\mathbf{X} = (\\mathbf{x}^1, \\mathbf{x}^2, \\dots, \\mathbf{x}^T)$. Each spectral vector $\\mathbf{x}^t \\in \\mathbb{R}^{k}$ at time-step $t$ is extracted from a short speech segment (~25 milliseconds) with the assumption that the signal is time-invariant in this small time window. Here, $k$ is the number of frequency bands in the [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) and this is a parameter of the feature extraction pipeline. The representation is based on the Fourier transform to convert the temporal signal into the frequency domain. \n",
    "\n",
    "In automatic speech recognition (ASR) research and applications, spectral vectors are usually referred to as \"acoustic frames\". Morover, adjacent frames are extracted with some overlap between them, usually ~10 milliseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41492d",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1674212543217,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "ef41492d"
   },
   "outputs": [],
   "source": [
    "def extract_melspectrogram(signal, sr, num_mels):\n",
    "    \"\"\"\n",
    "    Given a time series speech signal (.wav), sampling rate (sr), \n",
    "    and the number of mel coefficients, return a mel-scaled \n",
    "    representation of the signal as numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    mel_features = librosa.feature.melspectrogram(y=signal,\n",
    "        sr=sr,\n",
    "        n_fft=200, # with sampling rate = 8000, this corresponds to 25 ms\n",
    "        hop_length=80, # with sampling rate = 8000, this corresponds to 10 ms\n",
    "        n_mels=num_mels, # number of frequency bins, use either 13 or 39\n",
    "        fmin=50, # min frequency threshold\n",
    "        fmax=4000 # max frequency threshold, set to SAMPLING_RATE/2\n",
    "    )\n",
    "    \n",
    "    # for numerical stability added this line\n",
    "    mel_features = np.where(mel_features == 0, np.finfo(float).eps, mel_features)\n",
    "\n",
    "    # 20 * log10 to convert to log scale\n",
    "    log_mel_features = 20*np.log10(mel_features)\n",
    "\n",
    "    # feature scaling\n",
    "    scaled_log_mel_features = preprocessing.scale(log_mel_features, axis=1)\n",
    "    \n",
    "    return scaled_log_mel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600d78e",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1674212543218,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "3600d78e"
   },
   "outputs": [],
   "source": [
    "melspectrogram = extract_melspectrogram(x, sr, num_mels=13)\n",
    "\n",
    "melspectrogram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0a639",
   "metadata": {
    "id": "7ae0a639"
   },
   "source": [
    "Note that the shape of the array (K x T) represents the number of frequency bands (K) and the number of spectral vectors in this representation (here, K=13, T=43). K is a hyperparameter and the recommended values in ASR research are (13, 39, 81, etc). Here, we fix K = 13. On the other hand, T varies from sample to sample depending on the duration of the sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e8976",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1674212543219,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "f17e8976"
   },
   "outputs": [],
   "source": [
    "# plot and view the spectrogram\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 2), sharex=True)\n",
    "\n",
    "img = librosa.display.specshow(\n",
    "    melspectrogram, \n",
    "    sr=sr, \n",
    "    x_axis='time', \n",
    "    y_axis='mel', \n",
    "    cmap='viridis', \n",
    "    fmax=4000, \n",
    "    hop_length=80\n",
    ")\n",
    "\n",
    "ax.set(title='Log-frequency power spectrogram')\n",
    "\n",
    "ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c872b6",
   "metadata": {
    "id": "e6c872b6"
   },
   "source": [
    "As you can see above from the figure, the spectrogram representation can be viewed as a matrix $\\mathbf{X} \\in \\mathbb{R}^{T} \\times \\mathbb{R}^{k}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5392b92",
   "metadata": {
    "id": "d5392b92"
   },
   "source": [
    "## Task I\n",
    "1. One problem with the spectrogram as a speech feature represetation is that different speech samples would have dfferent durations due to inherent speech variability (e.g., speech rate, speaker dialect, etc). That is, the $T$ in the $(T \\times k)$-dimensional representation would be different for each sample. Therefore, for the baseline model, we will implement a method to have a fixed-size representation for all speech samples. Write a function downsample_spectrogram(X, N) that takes as input a spectrogram $\\mathbf{X} \\in \\mathbb{R}^{T \\times k}$ and a parameter N <= 25. The function should (1) make N equally-sized splits of S across the time-axis, (2) apply a pooling technique (e.g., mean pooling) to each split across the frequency axis to obtain an array that represents a downsampled version of the spectrogram $\\mathbf{X}' \\in \\mathbb{R}^{N \\times k}$, and (3) re-arange $\\mathbf{X}'$ as a vector $\\mathbf{v} \\in \\mathbb{R}^{Nk}$.    \n",
    "\n",
    "2. Using the downsample_spectrogram(X, N) function, transform all the speech samples into vectors $\\mathbf{v} \\in \\mathbb{R}^{Nk}$. \n",
    "\n",
    "3. Given the speaker-based train/dev/test spilts in the SDR_metadata.tsv, fit a linear model on the training samples. That is, your model should be build on data from 4 speakers {'nicolas', 'theo' , 'jackson',  'george'}. Hint: you can experiment with a few model alternatives in the SGDClassifier module in scikit-learn. \n",
    "\n",
    "4. Evaluate you model on the dev and test splits. Use accuracy as an evaluation metric. Analyze the model performance using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of the all possible labels (0-9), Analyze [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall), [F1-score](https://en.wikipedia.org/wiki/F-score) for each label. Report your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a3e64",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1674212543220,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "423a3e64"
   },
   "outputs": [],
   "source": [
    "def downsample_spectrogram(X, N):\n",
    "    \"\"\"\n",
    "    Given a spectrogram of an arbitrary length/duration (X ∈ K x T), \n",
    "    return a downsampled version of the spectrogram v ∈ K * N\n",
    "    \"\"\"\n",
    "    # ... your code here\n",
    "    pass \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe63ed",
   "metadata": {
    "id": "1bbe63ed"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53298ad",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "aborted",
     "timestamp": 1674212543221,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "c53298ad"
   },
   "outputs": [],
   "source": [
    "# prepare data and split \n",
    "\n",
    "# train a linear model \n",
    "\n",
    "# evaluate the model using accuracy metric\n",
    "\n",
    "# analyze the confusion matrix of the baseline \n",
    "\n",
    "# report precision, recall, F1-score for each label "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d62c9",
   "metadata": {
    "id": "435d62c9"
   },
   "source": [
    "## Task II\n",
    "1. Having established a baseline with a linear model trained on a downsampled signal representation of the speech segment, this task aims to learn a classifier based on the full speech segment. To this end, you will implement a neural model that is suitable for sequential data such as recurrent DNN, convolutional DNN with 1-D temporal convolution, or an audio transformer. The model should take the acoustic sample as it is (i.e., the Mel spectrogram could have an arbitrary length) without the need to downsample the segment. You need to implement at least two of the aforementioned models. Do the neural models improve accuracy over the baseline model? Do you observe any signs of overfitting to the training data? How do the hyperparameters affect the model performance? Report and discuss your observations.        \n",
    "\n",
    "2. Evaluate your (best) neural models and compare to the baseline model using the same evalution process as in task I.4. \n",
    "\n",
    "3. Use a dimensionality reduction algorithm such as t-SNE \\[[1](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding),[2](https://pypi.org/project/tsne-torch/),[3](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\\] or [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) to analyze how the different models seperate the different classes (the last non-linear layer in your model). Compare to the downsampled representation you used in the baseline and report your observations.\n",
    "\n",
    "4. Are the differences between the different models statistically significant? To answer this question, you need to implement a statistical significance test based on bootstrapping method. To read more how to estiame p-values based on bootstrapping, we recommend the materials on this paper https://aclanthology.org/D12-1091.pdf. Include the baseline model in your evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95e312",
   "metadata": {
    "id": "8f95e312"
   },
   "source": [
    "## Task III (Open Ended)\n",
    "1. Consider the case where we have speech data from a single speaker (e.g., george). Train your models on this subset of the data. What do you observe? How does this affect the model performance? \n",
    "\n",
    "2. Even though a model is trained on a single speaker, we would like the model to generalizes to any speaker. To this end, one can use data augmentation techniques to artificially create more samples for each class. Some of these augmentations can be applied on the spectrogram (e.g., SpecAugment https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html), and other can be applied on the raw waveform before creating the spectrogram such as pitch manipulation (https://github.com/facebookresearch/WavAugment). Explore the effect of one type of augmentation from each type. Report your observation and anaylze the confusion matrices.\n",
    "\n",
    "3. Data augmentation techniques create different \"views\" of each training sample in a stochastic or determinstic approach. One can leaverage speech data augmentation to create views for training a neural network in a contrastive learning setting with margin-based objective function (for more info, read http://proceedings.mlr.press/v130/al-tahan21a/al-tahan21a.pdf). Implement at least one model using a contrastive loss based on different views of the training samples. Does this model improve over the model without contrastive learning? Report and discuss your observations. \n",
    "\n",
    "For more information on the contrastive learning framework, you can refer to this paper\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9226466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33674cd9",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1674212543222,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "33674cd9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab3876",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1674212543223,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "6aab3876"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c37dff",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1674212543224,
     "user": {
      "displayName": "Hyoseung Kang",
      "userId": "00233629824676120159"
     },
     "user_tz": -60
    },
    "id": "34c37dff"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
